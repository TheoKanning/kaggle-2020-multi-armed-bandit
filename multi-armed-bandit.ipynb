{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santa 2020 - The Candy Cane Contest\n",
    "\n",
    "Bayesian UCB agent inspired by https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html  \n",
    "Thompson Sampling agent inspired by https://www.kaggle.com/ilialar/simple-multi-armed-bandit  \n",
    "Training data collection inspired by https://www.kaggle.com/lebroschar/1000-greedy-decision-tree-model  \n",
    "Ray support from https://www.kaggle.com/nigelcarpenter/parallel-processing-agent-trials-using-ray  \n",
    "\n",
    "Ideas:\n",
    "- Add more data and see what happens\n",
    "- Test agents trained on varying amounts of data\n",
    "- Add training data from agents around my ranking\n",
    "- Try ensemble model\n",
    "- Try nn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ray\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "print(f\"Initializing ray with {num_cpus} cpus\")\n",
    "ray.shutdown()\n",
    "ray.init(num_cpus=num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.stats\n",
    "\n",
    "def get_wins(scores):\n",
    "    # returns a tuple of win counts (p1 wins, p2 wins)\n",
    "    scores = np.array(scores)\n",
    "    return np.sum(scores[:,0] > scores[:,1]), np.sum(scores[:,1] > scores[:,0])\n",
    "\n",
    "def get_los_from_scores(scores):\n",
    "    p1_wins, p2_wins = get_wins(scores)\n",
    "    return get_los(p1_wins, p2_wins)\n",
    "\n",
    "def get_los(p1_wins, p2_wins):\n",
    "    # calculate likelihood of superiority for player 1 based on win counts\n",
    "    # the LOS for player 2 is the complement\n",
    "    if p1_wins == 0:\n",
    "        return 0\n",
    "    if p2_wins == 0:\n",
    "        return 1\n",
    "\n",
    "    return scipy.stats.beta(p1_wins, p2_wins).sf(0.5)\n",
    "\n",
    "def print_inline_stats(scores):\n",
    "    p1_wins, p2_wins = get_wins(scores)\n",
    "    p1_los = get_los(p1_wins, p2_wins)\n",
    "    \n",
    "    print(f\"Results after {len(scores)} games: {p1_wins}-{p2_wins} LOS: {p1_los:.3f}\", end='\\r')\n",
    "\n",
    "def print_stats(scores):\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    p1_mean, p2_mean = np.average(scores, axis=0)\n",
    "    p1_wins, p2_wins = get_wins(scores)\n",
    "    \n",
    "    p1_los = get_los(p1_wins, p2_wins)\n",
    "    p2_los = 1 - p1_los\n",
    "\n",
    "    print(f\"Wins: {p1_wins:5} {p2_wins:5}\")\n",
    "    print(f\"Mean: {p1_mean:.1f} {p2_mean:.1f}\")\n",
    "    print(f\"LOS:  {p1_los:0.3f} {p2_los:.3f}\")\n",
    "\n",
    "print_stats([[100,110],[120,125],[150,125]])\n",
    "print_inline_stats([[100,110],[120,125],[150,125]])\n",
    "print_inline_stats([[100,110],[120,125],[150,125]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile random_agent.py\n",
    "\n",
    "import random\n",
    "\n",
    "class RandomAgent:\n",
    "    \n",
    "    def step(self, observation, configuration):\n",
    "        return random.randrange(configuration.banditCount)\n",
    "    \n",
    "    def description(self):\n",
    "        return \"Random Agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Thompson Sampling Agent\n",
    "\n",
    "Inspired by https://www.kaggle.com/ilialar/simple-multi-armed-bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %%writefile thompson.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class ThompsonAgent():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bandit_states = None\n",
    "        self.last_action = None\n",
    "        self.total_reward = 0\n",
    "    \n",
    "    def name(self):\n",
    "        return \"Thompson Agent\"\n",
    "    \n",
    "    def description(self):\n",
    "        return self.name()\n",
    "    \n",
    "    def step(self, observation, configuration):\n",
    "        if observation.step == 0:\n",
    "            self.bandit_states = np.ones((configuration.banditCount, 2))\n",
    "        else:\n",
    "            player = observation.agentIndex\n",
    "            opponent = 1 if player == 0 else 0\n",
    "        \n",
    "            reward = observation.reward - self.total_reward\n",
    "            self.total_reward = observation.reward\n",
    "        \n",
    "            if reward:\n",
    "                self.bandit_states[self.last_action][0] += 1\n",
    "            else:\n",
    "                self.bandit_states[self.last_action][1] += 1\n",
    "        \n",
    "            for bandit in observation.lastActions:\n",
    "                self.bandit_states[bandit][0] *= 0.97\n",
    "\n",
    "        probs = np.random.beta(self.bandit_states[:,0], self.bandit_states[:,1])\n",
    "        best_bandit = int(np.argmax(probs))\n",
    "    \n",
    "        self.last_action = best_bandit\n",
    "    \n",
    "        return best_bandit\n",
    "            \n",
    "agent = ThompsonAgent()\n",
    "\n",
    "def thompson_agent(observation, configuration):\n",
    "    return agent.step(observation, configuration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian UCB Agent\n",
    "\n",
    "https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile bayesian_ucb_with_02_opp.py\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "# todo replace self.lastAction with lastActions[agentIndex]\n",
    "class UcbAgent:\n",
    "    \n",
    "    def __init__(self, c=3, decay=1.0, opp_bonus=0, random=False):\n",
    "        self.wins = None\n",
    "        self.losses = None\n",
    "        self.num_bandits = None\n",
    "        self.last_action = 0\n",
    "        self.opp_actions = []\n",
    "        self.total_reward = 0\n",
    "\n",
    "        # parameters\n",
    "        self.c = c # number of standard deviations in confidence interval\n",
    "        self.decay = decay\n",
    "        self.opp_bonus = opp_bonus\n",
    "        self.random = random\n",
    "        \n",
    "    def name(self):\n",
    "        return \"UCB Agent\"\n",
    "    \n",
    "    def description(self):\n",
    "        return f\"UCB Agent c={self.c} decay={self.decay:.2f} opp_bonus={self.opp_bonus} random={self.random}\"\n",
    "    \n",
    "    def step(self, observation, configuration) -> int:\n",
    "        if observation.step == 0:\n",
    "            self.num_bandits = configuration.banditCount\n",
    "            self.wins = np.ones(self.num_bandits)\n",
    "            if self.random:\n",
    "                self.wins += np.random.rand(self.num_bandits)/100\n",
    "            self.losses = np.ones(self.num_bandits)\n",
    "        else:\n",
    "            player = observation.agentIndex\n",
    "            opponent = 1 if player == 0 else 0\n",
    "        \n",
    "            reward = observation.reward - self.total_reward\n",
    "            self.total_reward = observation.reward\n",
    "        \n",
    "            # adjust win or loss counts\n",
    "            if reward:\n",
    "                self.wins[self.last_action] += 1\n",
    "            else:\n",
    "                self.losses[self.last_action] += 1\n",
    "            \n",
    "            self.opp_bonus_adjustment(observation.lastActions[opponent])\n",
    "        \n",
    "            # update total pull counts and decay\n",
    "            if observation.step < 10:\n",
    "                for bandit in observation.lastActions:\n",
    "                    self.wins[bandit] *= self.decay\n",
    "        \n",
    "        ucbs = (self.wins)/(self.wins+self.losses) + self.c * scipy.stats.beta.std(self.wins, self.losses)\n",
    "        self.last_action = int(np.argmax(ucbs))\n",
    "        return self.last_action\n",
    "    \n",
    "    def opp_bonus_adjustment(self, opp_action):\n",
    "        # give a bonus to a bandit if our opponent tried it twice in a row\n",
    "\n",
    "        if self.opp_actions and self.opp_actions[-1] == opp_action:\n",
    "            # could add decay here but it probably won't matter because opp_bonus is arbitrary anyway\n",
    "            self.wins[opp_action] += self.opp_bonus\n",
    "        \n",
    "        self.opp_actions.append(opp_action)\n",
    "                \n",
    "agent = UcbAgent(\n",
    "    opp_bonus=0.2,\n",
    "    c=3\n",
    ")\n",
    "\n",
    "def ucb_agent(observation, configuration):\n",
    "    return agent.step(observation, configuration)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Vegas Agent\n",
    "\n",
    "I'm not going to submit this, but since it's from a popular notebook I need to train against it  \n",
    "https://www.kaggle.com/a763337092/pull-vegas-slot-machines-add-weaken-rate-continue5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, os, datetime, math\n",
    "from collections import defaultdict\n",
    "\n",
    "class PullVegasAgent:\n",
    "    \n",
    "    def __init__(self, num_bandits):\n",
    "        self.total_reward = 0\n",
    "        self.actions = []\n",
    "        self.opp_actions = [] \n",
    "        \n",
    "        self.wins = np.ones(num_bandits)\n",
    "        self.losses = np.zeros(num_bandits)\n",
    "        self.opp = np.zeros(num_bandits)\n",
    "        self.my_continue = np.zeros(num_bandits)\n",
    "        self.opp_continue = np.zeros(num_bandits)\n",
    "        \n",
    "    def description(self):\n",
    "        return \"Pull Vegas\"\n",
    "    \n",
    "    def step(self, observation, configuration):\n",
    "        return self.multi_armed_probabilities(observation, configuration)\n",
    "        \n",
    "    def get_next_bandit(self):\n",
    "        total_pulls = self.wins + self.losses + self.opp\n",
    "        probs = (self.wins - self.losses + self.opp - (self.opp>0)*1.5 + self.opp_continue) / (total_pulls) \\\n",
    "                    * np.power(0.97, total_pulls)\n",
    "        best_bandit = np.argmax(probs)\n",
    "        return best_bandit\n",
    "\n",
    "    def multi_armed_probabilities(self, observation, configuration):\n",
    "        \n",
    "        if observation.step == 0:\n",
    "            return random.randrange(configuration.banditCount)\n",
    "        \n",
    "        last_reward = observation.reward - self.total_reward\n",
    "        self.total_reward = observation.reward\n",
    "\n",
    "        my_idx = observation.agentIndex\n",
    "        my_last_action = observation.lastActions[my_idx]\n",
    "        opp_last_action = observation.lastActions[1-my_idx]\n",
    "\n",
    "        self.actions.append(my_last_action)\n",
    "        self.opp_actions.append(opp_last_action)\n",
    "\n",
    "        if last_reward:\n",
    "            self.wins[my_last_action] += 1\n",
    "        else:\n",
    "            self.losses[my_last_action] += 1\n",
    "                \n",
    "        self.opp[opp_last_action] += 1\n",
    "\n",
    "        if observation.step >= 3:\n",
    "            if self.actions[-1] == self.actions[-2]:\n",
    "                self.my_continue[my_last_action] += 1\n",
    "            else:\n",
    "                self.my_continue[my_last_action] = 0\n",
    "            if self.opp_actions[-1] == self.opp_actions[-2]:\n",
    "                self.opp_continue[opp_last_action] += 1\n",
    "            else:\n",
    "                self.opp_continue[opp_last_action] = 0\n",
    "\n",
    "        if last_reward:\n",
    "            return my_last_action\n",
    "\n",
    "        if observation.step >= 4:\n",
    "            if (self.actions[-1] == self.actions[-2]) and (self.actions[-1] == self.actions[-3]):\n",
    "                if random.random() < 0.5:\n",
    "                    return self.actions[-1]\n",
    "\n",
    "        return self.get_next_bandit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sklearn Regression Agent\n",
    "\n",
    "Uses sklearn regression model to predict the probability for each machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile sklearn_with_streak.py\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SklearnRegressionAgent():\n",
    "    \n",
    "    def __init__(self, num_bandits, filename, margin=0.99):\n",
    "        self.machine_states = pd.DataFrame(\n",
    "            index=range(num_bandits), \n",
    "            columns=['step', 'n_pulls', 'n_success', 'n_opp_pulls', 'streak', 'opp_streak']\n",
    "        ).fillna(0)\n",
    "        self.total_reward = 0\n",
    "        self.filename = filename\n",
    "        self.model = joblib.load(filename)\n",
    "        self.margin = margin\n",
    "        \n",
    "    def name(self):\n",
    "        return \"Sklearn Regression Agent\"\n",
    "    \n",
    "    def description(self):\n",
    "        return f\"Sklearn - {self.filename}, margin:{self.margin:.2f}\"\n",
    "    \n",
    "    def step(self, observation, configuration):\n",
    "        if observation.step == 0:\n",
    "            return np.random.randint(configuration.banditCount)\n",
    "        \n",
    "        reward = observation.reward - self.total_reward\n",
    "        self.total_reward = observation.reward\n",
    "        last_action = observation.lastActions[observation.agentIndex]\n",
    "        opp_action = observation.lastActions[1-observation.agentIndex]\n",
    "        \n",
    "        self.machine_states['step'] = observation.step\n",
    "        self.machine_states.at[last_action, 'n_pulls'] += 1\n",
    "        self.machine_states.at[last_action, 'n_success'] += reward\n",
    "        self.machine_states.at[opp_action, 'n_opp_pulls'] += 1\n",
    "        \n",
    "        self.machine_states.at[last_action, 'streak'] += 1\n",
    "        self.machine_states.loc[self.machine_states.index != last_action, 'streak'] = 0\n",
    "        self.machine_states.at[opp_action, 'opp_streak'] += 1\n",
    "        self.machine_states.loc[self.machine_states.index != opp_action, 'opp_streak'] = 0\n",
    "        \n",
    "        probs = self.model.predict(self.machine_states)\n",
    "        \n",
    "        max_return = np.max(probs)\n",
    "        result = np.random.choice(np.where(probs >= self.margin * max_return)[0])\n",
    "        return int(result)\n",
    "\n",
    "agent = None\n",
    "\n",
    "def regression_agent(observation, configuration):\n",
    "    global agent\n",
    "    if observation.step == 0:\n",
    "        print(\"Creating decision tree agent\")\n",
    "        agent = SklearnRegressionAgent(configuration.banditCount, \"/kaggle_simulations/agent/model.joblib\")\n",
    "    \n",
    "    return agent.step(observation, configuration)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Regression Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile keras_agent.py\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class KerasRegressionAgent():\n",
    "    \n",
    "    def __init__(self, num_bandits, filename, margin=0.99):\n",
    "        self.total_reward = 0\n",
    "        self.filename = filename\n",
    "        self.margin = margin\n",
    "\n",
    "        self.model = keras.models.load_model(self.filename)\n",
    "        self.machine_states = pd.DataFrame(\n",
    "            index=range(num_bandits), \n",
    "            columns=['step', 'n_pulls', 'n_success', 'n_opp_pulls', 'streak', 'opp_streak']\n",
    "        ).fillna(0)\n",
    "        \n",
    "    def name(self):\n",
    "        return \"Keras Regression Agent\"\n",
    "    \n",
    "    def description(self):\n",
    "        return f\"Keras - {self.filename}, margin:{self.margin:.2f}\"\n",
    "    \n",
    "    def step(self, observation, configuration):\n",
    "        if observation.step == 0:\n",
    "            return np.random.randint(configuration.banditCount)\n",
    "        \n",
    "        reward = observation.reward - self.total_reward\n",
    "        self.total_reward = observation.reward\n",
    "        last_action = observation.lastActions[observation.agentIndex]\n",
    "        opp_action = observation.lastActions[1-observation.agentIndex]\n",
    "        \n",
    "        self.machine_states['step'] = observation.step\n",
    "        self.machine_states.at[last_action, 'n_pulls'] += 1\n",
    "        self.machine_states.at[last_action, 'n_success'] += reward\n",
    "        self.machine_states.at[opp_action, 'n_opp_pulls'] += 1\n",
    "        \n",
    "        self.machine_states.at[last_action, 'streak'] += 1\n",
    "        self.machine_states.loc[self.machine_states.index != last_action, 'streak'] = 0\n",
    "        self.machine_states.at[opp_action, 'opp_streak'] += 1\n",
    "        self.machine_states.loc[self.machine_states.index != opp_action, 'opp_streak'] = 0\n",
    "        \n",
    "        probs = self.model(self.machine_states.to_numpy())\n",
    "        \n",
    "        max_return = np.max(probs)\n",
    "        result = np.random.choice(np.where(probs >= self.margin * max_return)[0])\n",
    "        return int(result)\n",
    "\n",
    "agent = None\n",
    "\n",
    "def regression_agent(observation, configuration):\n",
    "    global agent\n",
    "    if observation.step == 0:\n",
    "        print(\"Creating keras agent\")\n",
    "        agent = KerasRegressionAgent(configuration.banditCount, \"models/streak_model.h5\")\n",
    "    \n",
    "    return agent.step(observation, configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Regression Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ensemble.py\n",
    "\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class EnsembleRegressionAgent():\n",
    "    \n",
    "    def __init__(self, num_bandits, keras_file, scikit_file, margin=0.99, alpha=0.5):\n",
    "        self.total_reward = 0\n",
    "        self.filename = f\"{keras_file} {scikit_file}\"\n",
    "        self.margin = margin\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.keras_model = keras.models.load_model(keras_file)\n",
    "        self.scikit_model = joblib.load(scikit_file)\n",
    "        self.machine_states = pd.DataFrame(\n",
    "            index=range(num_bandits), \n",
    "            columns=['step', 'n_pulls', 'n_success', 'n_opp_pulls', 'streak', 'opp_streak']\n",
    "        ).fillna(0)\n",
    "        \n",
    "    def name(self):\n",
    "        return \"Ensemble Regression Agent\"\n",
    "    \n",
    "    def description(self):\n",
    "        return f\"Ensemble - {self.filename}, margin:{self.margin:.2f} alpha:{self.alpha}\"\n",
    "    \n",
    "    def step(self, observation, configuration):\n",
    "        if observation.step == 0:\n",
    "            return np.random.randint(configuration.banditCount)\n",
    "         \n",
    "        reward = observation.reward - self.total_reward\n",
    "        self.total_reward = observation.reward\n",
    "        last_action = observation.lastActions[observation.agentIndex]\n",
    "        opp_action = observation.lastActions[1-observation.agentIndex]\n",
    "        \n",
    "        self.machine_states['step'] = observation.step\n",
    "        self.machine_states.at[last_action, 'n_pulls'] += 1\n",
    "        self.machine_states.at[last_action, 'n_success'] += reward\n",
    "        self.machine_states.at[opp_action, 'n_opp_pulls'] += 1\n",
    "        \n",
    "        self.machine_states.at[last_action, 'streak'] += 1\n",
    "        self.machine_states.loc[self.machine_states.index != last_action, 'streak'] = 0\n",
    "        self.machine_states.at[opp_action, 'opp_streak'] += 1\n",
    "        self.machine_states.loc[self.machine_states.index != opp_action, 'opp_streak'] = 0\n",
    "        \n",
    "        probs = self.get_probs()\n",
    "        \n",
    "        max_return = np.max(probs)\n",
    "        result = np.random.choice(np.where(probs >= self.margin * max_return)[0])\n",
    "        return int(result)\n",
    "    \n",
    "    def get_probs(self):\n",
    "        keras_probs = self.keras_model(self.machine_states.to_numpy())\n",
    "        scikit_probs = self.scikit_model.predict(self.machine_states)\n",
    "        scikit_probs = np.reshape(scikit_probs, (-1, 1))\n",
    "        return self.alpha * keras_probs + (1 - self.alpha) * scikit_probs\n",
    "\n",
    "agent = None\n",
    "\n",
    "def regression_agent(observation, configuration):\n",
    "    global agent\n",
    "    if observation.step == 0:\n",
    "        print(\"Creating ensemble agent\")\n",
    "        agent = EnsembleRegressionAgent(configuration.banditCount, \n",
    "                                        keras_file=\"/kaggle_simulations/agent/model.h5\",\n",
    "                                        scikit_file=\"/kaggle_simulations/agent/model.joblib\")\n",
    "    \n",
    "    return agent.step(observation, configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoke_test(KerasRegressionAgent(100, filename='models/streak_model.h5'))\n",
    "smoke_test(SklearnRegressionAgent(100, filename='models/dtr_streak_model.joblib'))\n",
    "smoke_test(EnsembleRegressionAgent(100, keras_file='models/streak_model.h5', scikit_file='models/dtr_streak_model.joblib'))\n",
    "smoke_test(RandomAgent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp models/streak_model.h5 model.h5\n",
    "!cp models/dtr_streak_model.joblib model.joblib\n",
    "!cp ensemble.py main.py\n",
    "!tar cvfz ensemble.tar.gz main.py model.h5 model.joblib\n",
    "!rm model.h5\n",
    "!rm model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Configuration = namedtuple('Configuration', ['banditCount'])\n",
    "Observation = namedtuple('Observation', ['step', 'reward', 'agentIndex', 'lastActions'])\n",
    "\n",
    "def smoke_test(agent):\n",
    "    config = Configuration(banditCount=100)\n",
    "    obs = Observation(step=0, reward=0, agentIndex=0, lastActions=[])\n",
    "    action = agent.step(obs, config)\n",
    "    \n",
    "    obs = Observation(step=1, reward=0, agentIndex=0, lastActions=[action, 2])\n",
    "    action = agent.step(obs, config)\n",
    "    \n",
    "    obs = Observation(step=2, reward=1, agentIndex=0, lastActions=[action, 5])\n",
    "    action = agent.step(obs, config)\n",
    "\n",
    "@ray.remote\n",
    "def simulate_mab(agent_lambdas, num_steps=2000, num_bandits=100, game_id=0):\n",
    "    \n",
    "    config = Configuration(banditCount=num_bandits)\n",
    "    probs = np.random.rand(num_bandits)\n",
    "    lastActions = [0, 0]\n",
    "    totals = [0, 0]\n",
    "    agents = [l(num_bandits) for l in agent_lambdas]\n",
    "    \n",
    "    d = {'step':[], 'p1_total':[], 'p2_total':[]}\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        for j, agent in enumerate(agents):\n",
    "            obs = Observation(step=i, reward=totals[j], agentIndex=j, lastActions=lastActions)\n",
    "            choice = agent.step(obs, config)\n",
    "            totals[j] += np.random.rand() < probs[choice]\n",
    "            lastActions[j] = choice\n",
    "            \n",
    "        d['step'].append(i)\n",
    "        d['p1_total'].append(totals[0])\n",
    "        d['p2_total'].append(totals[1])\n",
    "        \n",
    "        for action in lastActions:\n",
    "            probs[action] *= 0.97\n",
    "\n",
    "    df = pd.DataFrame(data=d)\n",
    "    df['diff'] = df.p1_total - df.p2_total\n",
    "    df['game_id'] = game_id\n",
    "    \n",
    "    return totals, df\n",
    "\n",
    "def compare_agents(agent_lambdas, num_games=50, num_bandits=100, num_steps=2000, min_games=20):\n",
    "\n",
    "    num_cpus = 4\n",
    "    \n",
    "    scores = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, num_games, num_cpus):\n",
    "        result_ids = [simulate_mab.remote(\n",
    "            agent_lambdas, \n",
    "            num_steps=num_steps, \n",
    "            num_bandits=num_bandits, \n",
    "            game_id=i) for i in range(i, i+num_cpus)]\n",
    "\n",
    "        batch_results = ray.get(result_ids)\n",
    "\n",
    "        for score, game_df in batch_results:\n",
    "            scores.append(score)\n",
    "            df = df.append(game_df)\n",
    "\n",
    "        print_inline_stats(scores)\n",
    "        p1_los = get_los_from_scores(scores)\n",
    "        if len(scores) >= min_games and (p1_los < 0.03 or p1_los > 0.97):\n",
    "            break\n",
    "            \n",
    "    return scores, df\n",
    "\n",
    "def round_robin(agent_lambdas, num_games=50, num_bandits=100, num_steps=2000, min_games=20):\n",
    "    num_agents = len(agent_lambdas)\n",
    "    agent_names = [l(num_bandits).description() for l in agent_lambdas]\n",
    "    records = np.zeros((num_agents, 3), dtype='int32')\n",
    "    los_matrix = np.full((num_agents, num_agents), 0.5)\n",
    "    \n",
    "    print(f\"Starting round robin with {num_agents} agents:\")\n",
    "    for name in agent_names:\n",
    "        print(\"\\t\"+name)\n",
    "    print(\"\")\n",
    "    \n",
    "    for i in range(num_agents-1):\n",
    "        for j in range(i+1, num_agents):\n",
    "            print(f\"Starting new round:\\n\\t{agent_names[i]}\\n\\t{agent_names[j]}\")\n",
    "            scores, _ = compare_agents([agent_lambdas[i], agent_lambdas[j]], \n",
    "                                            num_games=num_games,\n",
    "                                            num_bandits=num_bandits, \n",
    "                                            num_steps=num_steps,\n",
    "                                            min_games=min_games)\n",
    "            p1_wins, p2_wins = get_wins(scores)\n",
    "            ties = len(scores) - p1_wins - p2_wins\n",
    "            records[i] += [p1_wins, p2_wins, ties]\n",
    "            records[j] += [p2_wins, p1_wins, ties]\n",
    "            \n",
    "            p1_los = get_los(p1_wins, p2_wins)\n",
    "            los_matrix[i,j] = p1_los\n",
    "            los_matrix[j,i] = 1 - p1_los\n",
    "            \n",
    "            print('\\n')\n",
    "    \n",
    "    for i in range(num_agents):\n",
    "        print(f\"{agent_names[i]}: {'-'.join(map(str, records[i]))}\")\n",
    "        \n",
    "    plot_los_heatmap(agent_names, los_matrix)\n",
    "            \n",
    "def graph_game_results(df):\n",
    "    ax = df.groupby('step').mean()['diff'].rolling(window=10).mean().plot(title=\"Point difference averages over all games\")\n",
    "    ax.set_xlabel(\"step\")\n",
    "    ax.set_ylabel(\"P1 - P2\")\n",
    "\n",
    "#     ax = df[df.game_id == 0][['p1_total', 'p2_total']].plot(title=\"Reward curves for game 0\")\n",
    "#     ax.set_xlabel(\"step\")\n",
    "#     ax.set_ylabel(\"reward\")\n",
    "\n",
    "def plot_los_heatmap(agent_names, los_matrix):\n",
    "    num_agents = len(agent_names)\n",
    "    \n",
    "    order = np.argsort(-np.sum(los_matrix, axis=1))\n",
    "    sorted_names = [agent_names[x] for x in order]\n",
    "    sorted_los = los_matrix[order][:,order]\n",
    "    \n",
    "    fix, ax =  plt.subplots()\n",
    "    ax.imshow(sorted_los, cmap='gray', vmin=0, vmax=1.5)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(num_agents))\n",
    "    ax.set_yticks(np.arange(num_agents))\n",
    "\n",
    "    # ... and label them with the respective list entries\n",
    "    ax.set_xticklabels(sorted_names)\n",
    "    ax.set_yticklabels(sorted_names)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        for j in range(num_agents):\n",
    "            if i == j:\n",
    "                continue\n",
    "            text = ax.text(j, i, \"{:.2f}\".format(sorted_los[i, j]),\n",
    "                           ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# agents = [\n",
    "#     lambda n: EnsembleRegressionAgent(n, keras_file='models/streak_model.h5', scikit_file='models/dtr_streak_model.joblib'),\n",
    "#     lambda n: KerasRegressionAgent(n, filename='models/streak_model.h5'),\n",
    "#     lambda n: SklearnRegressionAgent(n, filename='models/dtr_streak_model.joblib'),\n",
    "#     lambda n: UcbAgent(),\n",
    "#     lambda n: ThompsonAgent(),\n",
    "#     lambda n: RandomAgent(),\n",
    "# ]\n",
    "\n",
    "agents = [\n",
    "    lambda n: EnsembleRegressionAgent(n, alpha=0.5, keras_file='models/streak_model.h5', scikit_file='models/dtr_streak_model.joblib'),\n",
    "#     lambda n: EnsembleRegressionAgent(n, alpha=0.25, keras_file='models/streak_model.h5', scikit_file='models/dtr_streak_model.joblib'),\n",
    "#     lambda n: EnsembleRegressionAgent(n, alpha=0.75, keras_file='models/streak_model.h5', scikit_file='models/dtr_streak_model.joblib'),\n",
    "    lambda n: KerasRegressionAgent(n, filename='models/streak_model.h5'),\n",
    "    lambda n: SklearnRegressionAgent(n, filename='models/dtr_streak_model.joblib'),\n",
    "    lambda n: PullVegasAgent(100)\n",
    "]\n",
    "\n",
    "round_robin(agents, num_games=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(agents):\n",
    "    print(f\"P1: {agents[0](100).description()}\")\n",
    "    print(f\"P2: {agents[1](100).description()}\")\n",
    "    _, df = compare_agents(agents, num_games=100, min_games=100)\n",
    "    graph_game_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([agents[0], agents[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([agents[0], agents[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([agents[0], agents[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare([agents[2], agents[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet('training_data.parquet')\n",
    "print(f\"\\nLoaded {train_data.shape[0]} training rows\")\n",
    "\n",
    "X = train_data[['step', 'n_pulls', 'n_success', 'n_opp_pulls']]\n",
    "y = train_data['threshold']\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sklearn Models\n",
    "\n",
    "Take training data from top-tier games and find a model that predicts actual payout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def cross_val_rmse(regressor, X, y):\n",
    "    cv = -cross_val_score(regressor, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
    "    print(cv)\n",
    "    print(cv.mean())\n",
    "\n",
    "def feature_importance(regressor, X, y):\n",
    "    regressor.fit(X, y)\n",
    "    for name, score in zip(X.columns, regressor.feature_importances_):\n",
    "        print(name, score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "cross_val_rmse(lr, X, y)\n",
    "\n",
    "lr.fit(X, y)\n",
    "joblib.dump(dtr, 'lr_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor(min_samples_leaf=100)\n",
    "cross_val_rmse(dtr, X, y)\n",
    "feature_importance(dtr, X, y)\n",
    "dtr.fit(X, y)\n",
    "joblib.dump(dtr, 'models/dtr_streak_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=100, min_samples_leaf=20, max_depth=10)\n",
    "cross_val_rmse(rfr, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr = SVR(kernel='poly', degree=2, C=10, epsilon=0.5)\n",
    "cross_val_rmse(svr, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_model(n_hidden_layers=1, n_units=10, activation='sigmoid', input_size=4, learning_rate=0.01):\n",
    "    input_layer = Input(shape=(input_size,))\n",
    "\n",
    "    for i in range(n_hidden_layers):\n",
    "        if i == 0:\n",
    "            m = Dense(n_units, activation='relu')(input_layer)\n",
    "        else:\n",
    "            m = Dense(n_units, activation='relu')(m)\n",
    "            \n",
    "    m = Dense(1, activation=activation)(m)\n",
    "    \n",
    "    model = Model(inputs=[input_layer], outputs=m)\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# model = create_model()\n",
    "# model.fit(X, y, batch_size=10000, epochs=10, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_hidden_layers': [2, 3, 4],\n",
    "    'n_units': [3, 5, 8],\n",
    "    'activation': ['sigmoid'],\n",
    "    'batch_size': [20000]\n",
    "}\n",
    "\n",
    "keras_reg = KerasRegressor(create_model)\n",
    "search_cv = GridSearchCV(keras_reg, params, cv=3)\n",
    "search_cv.fit(X, y, epochs=50, batch_size=20000, callbacks=[EarlyStopping(patience=5)], validation_split=0.05)\n",
    "search_cv.best_params_ # {'activation': 'sigmoid', 'n_hidden_layers': 3, 'n_units': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(n_hidden_layers=3, n_units=8, activation='sigmoid', input_size=6, learning_rate=0.01)\n",
    "\n",
    "# train_data['log_step'] = np.log(train_data.step + 1)\n",
    "X = train_data[['step', 'n_pulls', 'n_success', 'n_opp_pulls']]\n",
    "# train_data\n",
    "# y = train_data['threshold']\n",
    "\n",
    "model.fit(X, y, batch_size=10000, epochs=100, validation_split=0.05, callbacks=[EarlyStopping(patience=5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred_nn = model.predict(X, batch_size=20000)\n",
    "y_pred_dtr = dtr.predict(X)\n",
    "y_pred_dtr = np.reshape(y_pred_dtr, (-1, 1))\n",
    "# model.save('models/model.h5')\n",
    "print(np.sqrt(mean_squared_error(y_pred_nn, y)))\n",
    "print(np.sqrt(mean_squared_error(y_pred_dtr, y)))\n",
    "print(np.sqrt(mean_squared_error((y_pred_nn+y_pred_dtr)/2, y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing in Kaggle Environment\n",
    "Not ideal for performance testing, but double-checks that they'll work online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "env = make(\"mab\", debug=True)\n",
    "\n",
    "env.reset()\n",
    "env.run([ \"decision_tree.py\", \"keras_agent.py\"])\n",
    "print(env)\n",
    "env.render(mode=\"ipython\", width=800, height=700)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@ray.remote\n",
    "def run_trial():\n",
    "    env = make(\"mab\")\n",
    "    env.reset()\n",
    "    env.run([\"bayesian_ucb_with_02_opp_and_rand.py\", \"decision_tree.py\"])\n",
    "    return env.state\n",
    "\n",
    "result_ids = [run_trial.remote() for i in range(10)]\n",
    "\n",
    "results = ray.get(result_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from kaggle_environments import evaluate\n",
    "\n",
    "results = np.array(evaluate(\"mab\", [\"keras_agent.py\", \"decision_tree.py\"], num_episodes=1))\n",
    "print_stats(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "klass = UcbAgent\n",
    "args = {'opp_bonus':0.2, 'decay':0.99}\n",
    "klass(**args).description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
